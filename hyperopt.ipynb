{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the data and create_model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.utils import multi_gpu_model, Sequence, np_utils\n",
    "import math\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Nadam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "\n",
    "def data():\n",
    "    embedding_file = 'opa2vec/backup/embeddings_extended_300.out'\n",
    "    data = pd.read_csv(embedding_file, header = None, sep = ' ', skiprows=0)\n",
    "    embds_data = data.values\n",
    "    patho_dict = dict(zip(embds_data[:,0],embds_data[:,1:]))\n",
    "\n",
    "    embedding_file = 'opa2vec2/backup/embeddings_HP_300.out'\n",
    "    data = pd.read_csv(embedding_file, header = None, sep = ' ', skiprows=0)\n",
    "    embds_data = data.values\n",
    "    host_dict = dict(zip(embds_data[:,0],embds_data[:,1:]))\n",
    "\n",
    "    protein_set = set()\n",
    "    with open('host_pheno_asso/human_pheno_asso_onlyhuman.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            protein_set.add(items[0])\n",
    "    \n",
    "    hosts = set()\n",
    "    pathos = set()\n",
    "    positives_set = set()\n",
    "\n",
    "    with open('./data/hpidb2.entrez.score.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split('\\t')\n",
    "            if ':' in items[2]:\n",
    "                if float(items[2].split('miscore:')[1]) > 0.5:\n",
    "                    patho = 'NCBITaxon_' + items[1]\n",
    "                    hosts.add(items[0])\n",
    "                    pathos.add(patho)\n",
    "                    positives_set.add((items[0], patho))\n",
    "\n",
    "    pathogen_missed = {}\n",
    "    positives = set()\n",
    "    for pair in positives_set:\n",
    "        if pair[0] in host_dict and pair[1] in patho_dict and pair[0] in protein_set:\n",
    "            positives.add((pair))\n",
    "        if pair[1] not in patho_dict:\n",
    "            if pair[1] not in pathogen_missed:\n",
    "                pathogen_missed[pair[1]] = 0\n",
    "            pathogen_missed[pair[1]] += 1\n",
    "        \n",
    "    return pathogens, positives, protein_set\n",
    "\n",
    "def create_model(pathogens, positives, protein_set):\n",
    "    \n",
    "    class Generator(Sequence):\n",
    "        def __init__(self, x_set, y_set, batch_size):\n",
    "            self.x, self.y = x_set, y_set\n",
    "            self.batch_size = batch_size\n",
    "            self.nbatch = int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "            self.length = len(self.x)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.nbatch\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            start = idx * self.batch_size\n",
    "            batch_len = min(self.batch_size, (self.length)-start)\n",
    "            X_batch_list = np.empty((batch_len, 600), dtype=np.float32)\n",
    "            y_batch_list = np.empty(batch_len, dtype=np.float32)\n",
    "\n",
    "            for ids in range(start, min((idx + 1) * self.batch_size, self.length)):\n",
    "                array1 = host_dict[self.x[ids][0]]\n",
    "                array2 = patho_dict[self.x[ids][1]]\n",
    "                embds = np.concatenate([array1, array2])\n",
    "                X_batch_list[ids-start] = embds\n",
    "                y_batch_list[ids-start] = self.y[ids]\n",
    "            return X_batch_list, y_batch_list\n",
    "    batch_size = 2**11\n",
    "    num_classes = 1\n",
    "    rank_counts = []\n",
    "    epochs = 5\n",
    "    for i in range(epochs):\n",
    "        rank_counts.append(dict())\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units={{choice([512, 256, 128, 64, 32])}}, activation={{choice(['relu', 'sigmoid'])}}, input_shape=(600,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate={{uniform(0, 1)}}))\n",
    "    if {{choice(['three', 'four'])}} == 'four':\n",
    "        model.add(Dense(units={{choice([64, 32, 16, 8])}}, activation={{choice(['relu', 'sigmoid'])}}))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate = {{uniform(0, 1)}}))\n",
    "        if {{choice(['three', 'four'])}} == 'three':\n",
    "            model.add(Dense(units={{choice([8, 4, 2])}}, activation={{choice(['relu', 'sigmoid'])}}))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(rate={{uniform(0, 1)}}))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer={{choice(['rmsprop', 'adam'])}},\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.save('model_300.h5')\n",
    "    pathogens = np.unique(np.array(list(positives))[:,1])\n",
    "    counter = 0\n",
    "    \n",
    "    for taxon in pathogens:\n",
    "        K.clear_session()\n",
    "        parallel_model = load_model('model_300.h5')\n",
    "        counter+=1\n",
    "        print('taxon ', counter)\n",
    "        val_pathos = set()\n",
    "        val_pathos.add(taxon)\n",
    "        train_pathos = set(list(pathogens)) - val_pathos\n",
    "\n",
    "        train_positives = []\n",
    "        val_positives = []\n",
    "        train_positives_set = set()\n",
    "        val_positives_set = set()\n",
    "        for items in positives:\n",
    "            if items[1] in train_pathos:\n",
    "                train_positives_set.add((items[0], items[1]))\n",
    "                train_positives.append((items[0], items[1], 1))\n",
    "            if items[1] in val_pathos:\n",
    "                val_positives_set.add((items[0], items[1]))\n",
    "                val_positives.append((items[0], items[1], 1))\n",
    "        print(len(train_positives), len(val_positives))\n",
    "\n",
    "        train_negatives = []\n",
    "        train_all_tuples = set()\n",
    "        pathogens1 = np.array(list(train_positives))[:,1]\n",
    "        for host in protein_set:\n",
    "            for patho in pathogens1:\n",
    "                if host in host_dict and patho in patho_dict:\n",
    "                    train_all_tuples.add((host, patho))\n",
    "        for item in train_all_tuples:\n",
    "            if item not in train_positives_set:\n",
    "                train_negatives.append((item[0], item[1], 0))\n",
    "\n",
    "        train_positives = np.repeat(np.array(list(train_positives)), len(train_negatives)//len(train_positives), axis = 0)\n",
    "        train_negatives = np.array(list(train_negatives))\n",
    "        triple_train = np.concatenate((train_positives, train_negatives), axis=0)\n",
    "        np.random.shuffle(triple_train)\n",
    "\n",
    "        factor = 1\n",
    "        generator = Generator(triple_train[:int(factor*len(triple_train)),0:2], triple_train[:int(factor*len(triple_train)),2], batch_size)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            history = parallel_model.fit_generator(generator=generator,\n",
    "                                epochs=1,\n",
    "                                steps_per_epoch = int(math.ceil(math.ceil(factor*len(triple_train))/ batch_size)),\n",
    "                                verbose=0,\n",
    "                                validation_data=generator,\n",
    "                                validation_steps=1, max_queue_size = 20,\n",
    "                                use_multiprocessing=True,\n",
    "                                workers = 6)\n",
    "\n",
    "            for pathogen in val_pathos:\n",
    "                protein_list = []\n",
    "                positive_set = set()\n",
    "                for items in positives:\n",
    "                    if items[1] == pathogen:\n",
    "                        protein_list.append((items[0], items[1], 1))\n",
    "                        positive_set.add(items[0])\n",
    "                num_positive = len(protein_list)\n",
    "                for protein in protein_set:\n",
    "                    if protein not in positive_set:\n",
    "                        protein_list.append((protein, pathogen, 0))\n",
    "                protein_list = np.array(protein_list)\n",
    "                sim_list = parallel_model.predict_generator(generator=Generator(protein_list[:,0:2], protein_list[:,2], 1000), verbose=0, steps=int(math.ceil(math.ceil(len(protein_list))  / 1000)), max_queue_size = 20, use_multiprocessing=True, workers = 6)\n",
    "                y_rank = ss.rankdata(-sim_list, method='average')\n",
    "                x_list = y_rank[:num_positive]\n",
    "                print(np.mean(x_list))\n",
    "                for x in x_list:\n",
    "                    if x not in rank_counts[i]:\n",
    "                        rank_counts[i][x] = 0\n",
    "                    rank_counts[i][x]+=1\n",
    "    aucs = []\n",
    "    for i in range(epochs):                \n",
    "        auc_x = list(rank_counts[i].keys())\n",
    "        auc_x.sort()\n",
    "        auc_y = []\n",
    "        tpr = 0\n",
    "        step = 1/sum(rank_counts[i].values())\n",
    "        for x in auc_x:\n",
    "            tpr += rank_counts[i][x]*step\n",
    "            auc_y.append(tpr)\n",
    "        auc_x.append(len(protein_set))\n",
    "        auc_y.append(1)\n",
    "        auc1 = np.trapz(auc_y, auc_x)/len(protein_set)\n",
    "        print('Rank based auc is: %f' % (auc1))\n",
    "        aucs.append(auc1)\n",
    "    max_auc = max(aucs)\n",
    "    output = open('hyperopt_300.aucs', 'a+')\n",
    "    output.write(str(aucs) + '\\n')\n",
    "    return {'loss': -max_auc, 'status': STATUS_OK, 'model': parallel_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "    best_run, best_model = optim.minimize(model=create_model, data=data, algo=tpe.suggest, max_evals=100, trials=Trials(), notebook_name='hyperopt_300')\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
